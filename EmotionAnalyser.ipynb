{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import codecs\n",
    "import jieba.posseg as pseg \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Words processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "# 创建停用词list\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "# 对句子进行分词\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stopwords = stopwordslist('stoped.txt')  # 这里加载停用词的路径\n",
    "    outstr = ''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分句子\n",
    "# 版本为python3，如果为python2需要在字符串前面加上u\n",
    "import re\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('([，！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    \n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Emotion corpus reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_df = pd.read_excel('情感词典修改版.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "em_dict = em_df[['词语','情感分类']].set_index('词语').T.to_dict(orient=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotion_words = list(em_df['词语'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotions_analyze(file_path, em_dict):\n",
    "    result_emotion = []\n",
    "    result_words = []\n",
    "    text = open(file_path, 'r',encoding=\"utf-8\") #加载要处理的文件的路径\n",
    "    for line in text: #这个循环是遍历整个文本，inputs是整个文本，line是文本中的每一段\n",
    "        l=cut_sent(line)   # l是给一段文本进行拆分句子后的结果\n",
    "        for w in l:  #这个循环是遍历一段文字里的每个句子,l是一段里的所有句子存放的list，w是每个句子\n",
    "            line_seg = seg_sentence(w)  #line_seg 是一个句子拆分词语后的结果\n",
    "            words=list(line_seg.split(' ')) #将一个句子中的词语拆分后逐个存放在list中\n",
    "            for w in words:\n",
    "                if w in list(em_dict):\n",
    "                    result_words.append(w) #筛选出情感词\n",
    "                    result_emotion.append(em_dict[w]) #匹配情感\n",
    "                    #i+=1\n",
    "    return result_words, result_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = './dataset/stories/'\n",
    "files = os.listdir(file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_words, result_emotion = emotions_analyze(file_path, em_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_results_process(result_emotion):\n",
    "    result_df = pd.DataFrame(data = result_emotion)\n",
    "    result_df.rename(columns={0:'emotions'}, inplace = True)\n",
    "    # .size() 聚合统计 .reset_index() 把index变成正常的列\n",
    "    new_df = result_df.groupby('emotions').size().reset_index()\n",
    "    new_df.rename(columns={0: 'times'}, inplace = True)\n",
    "    print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emotions  times\n",
      "0        NB     20\n",
      "1        NC     14\n",
      "2        ND      8\n",
      "3        NE     13\n",
      "4        NH      5\n",
      "5        NI      3\n",
      "6        NJ      4\n",
      "7        NL     14\n",
      "8        NN     48\n",
      "9        PA     37\n",
      "10       PB      8\n",
      "11       PC      5\n",
      "12       PD      4\n",
      "13       PE      9\n",
      "14       PF      2\n",
      "15       PG      9\n",
      "16       PH     58\n",
      "17      PH      28\n",
      "18       PK      6\n",
      "   emotions  times\n",
      "0        NB      4\n",
      "1        NC      1\n",
      "2        ND      8\n",
      "3        NE     16\n",
      "4        NL      4\n",
      "5        NN     15\n",
      "6        PA      9\n",
      "7        PB     11\n",
      "8        PD      4\n",
      "9        PE      1\n",
      "10       PF      5\n",
      "11       PG      1\n",
      "12       PH     38\n",
      "13      PH       4\n",
      "14       PK      6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8a0f1f03d2dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./dataset/new_data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotions_analyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0memotion_results_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_emotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(result_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7ef56da1c3a6>\u001b[0m in \u001b[0;36memotions_analyze\u001b[0;34m(file_path, em_dict)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_seg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#将一个句子中的词语拆分后逐个存放在list中\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                     \u001b[0mresult_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#筛选出情感词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mresult_emotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#匹配情感\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    file_path = file_dir+file\n",
    "    result_words, result_emotion = emotions_analyze(file_path, em_dict)\n",
    "    emotion_results_process(result_emotion)\n",
    "    #print(result_words)\n",
    "    #print(result_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['雄伟', '美丽', '漂亮', '喜欢', '盛大', '祝贺', '威风凛凛', '潇洒', '英俊', '大吃一惊', '细心', '微笑', '伤心', '自言自语', '帮助', '希望', '美妙', '好', '难过', '分裂', '华丽', '美人', '称赞', '美妙', '取乐', '无微不至', '伤心', '哭泣', '宝剑', '不了', '自言自语', '五彩缤纷', '照耀', '可爱', '满足']\n",
      "[['PH'], ['PH'], ['PH'], ['PB'], ['PH'], ['PA'], ['PH'], ['PH'], ['PH'], ['PC'], ['PH'], ['PA'], ['NB'], ['NN'], ['PB'], ['PK'], ['PH'], ['PH '], ['NB'], ['NC'], ['PH'], ['PH'], ['PH'], ['PH'], ['PA'], ['PH'], ['NB'], ['NB'], ['PH'], ['NL'], ['NN'], ['PB'], ['PA'], ['PH'], ['PA']]\n"
     ]
    }
   ],
   "source": [
    "print(result_words)\n",
    "print(result_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotions</th>\n",
       "      <th>times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PB</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PH</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotions  times\n",
       "0       NB      4\n",
       "1       NC      1\n",
       "2       NL      1\n",
       "3       NN      2\n",
       "4       PA      5\n",
       "5       PB      3\n",
       "6       PC      1\n",
       "7       PH     16\n",
       "8      PH       1\n",
       "9       PK      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
