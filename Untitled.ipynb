{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(file_name):\n",
    "    with open(file_name,\"r\") as fp:\n",
    "        words = fp.read()\n",
    "    return words\n",
    "\n",
    "def stop_words(stop_word_file):\n",
    "    words = read_from_file(stop_word_file)\n",
    "    result = jieba.cut(words)\n",
    "    new_words = []\n",
    "    for r in result:\n",
    "        new_words.append(r)\n",
    "    return set(new_words)\n",
    "def del_stop_words(words,stop_words_set):\n",
    "#   words是已经切词但是没有去除停用词的文档。\n",
    "#   返回的会是去除停用词后的剩余词\n",
    "    result = jieba.cut(words)\n",
    "    new_words = []\n",
    "    for r in result:\n",
    "        if r not in stop_words_set:\n",
    "            new_words.append(r)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_vector(file_path,stop_words_set):\n",
    "    names = [ os.path.join(file_path,f) for f in os.listdir(file_path) ]\n",
    "    posts = [ open(name).read() for name in names ]\n",
    "    docs = []\n",
    "    word_set = set()\n",
    "    for post in posts:\n",
    "        doc = del_stop_words(post,stop_words_set)\n",
    "        docs.append(doc)\n",
    "        word_set |= set(doc)\n",
    "        #print len(doc),len(word_set)\n",
    "\n",
    "    word_set = list(word_set)\n",
    "    docs_vsm = []\n",
    "    #for word in word_set[:30]:\n",
    "        #print word.encode(\"utf-8\"),\n",
    "    for doc in docs:\n",
    "        temp_vector = []\n",
    "        for word in word_set:\n",
    "            temp_vector.append(doc.count(word) * 1.0)\n",
    "        #print temp_vector[-30:-1]\n",
    "        docs_vsm.append(temp_vector)\n",
    "    docs_matrix = np.array(docs_vsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sum = [ float(len(np.nonzero(docs_matrix[:,i])[0])) for i in range(docs_matrix.shape[1]) ]\n",
    "column_sum = np.array(column_sum)\n",
    "column_sum = docs_matrix.shape[0] / column_sum\n",
    "idf =  np.log(column_sum)\n",
    "idf =  np.diag(idf)\n",
    "# 根据IDF的定义，计算词的IDF并不依赖于某个文档，所以我们提前计算好。\n",
    "# 注意一下计算都是矩阵运算，不是单个变量的运算。\n",
    "for doc_v in docs_matrix:\n",
    "    if doc_v.sum() == 0:\n",
    "        doc_v = doc_v / 1\n",
    "    else:\n",
    "        doc_v = doc_v / (doc_v.sum())\n",
    "    tfidf = np.dot(docs_matrix,idf)\n",
    "    return names,tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim(A,B):\n",
    "    num = float(np.dot(A,B.T))\n",
    "    denum = np.linalg.norm(A) * np.linalg.norm(B)\n",
    "    if denum == 0:\n",
    "        denum = 1\n",
    "    cosn = num / denum\n",
    "    sim = 0.5 + 0.5 * cosn\n",
    "    return sim\n",
    "def randCent(dataSet, k):\n",
    "    n = shape(dataSet)[1]\n",
    "    centroids = mat(zeros((k,n)))#create centroid mat\n",
    "    for j in range(n):#create random cluster centers, within bounds of each dimension\n",
    "        minJ = min(dataSet[:,j]) \n",
    "        rangeJ = float(max(dataSet[:,j]) - minJ)\n",
    "        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1))\n",
    "    return centroids\n",
    "    \n",
    "def kMeans(dataSet, k, distMeas=gen_sim, createCent=randCent):\n",
    "    m = shape(dataSet)[0]\n",
    "    clusterAssment = mat(zeros((m,2)))#create mat to assign data points \n",
    "                                      #to a centroid, also holds SE of each point\n",
    "    centroids = createCent(dataSet, k)\n",
    "    clusterChanged = True\n",
    "    counter = 0\n",
    "    while counter <= 50:\n",
    "        counter += 1\n",
    "        clusterChanged = False\n",
    "        for i in range(m):#for each data point assign it to the closest centroid\n",
    "            minDist = inf; \n",
    "            minIndex = -1\n",
    "            for j in range(k):\n",
    "                distJI = distMeas(centroids[j,:],dataSet[i,:])\n",
    "                if distJI < minDist:\n",
    "                    minDist = distJI; \n",
    "                    minIndex = j\n",
    "            if clusterAssment[i,0] != minIndex: \n",
    "                clusterChanged = True\n",
    "            clusterAssment[i,:] = minIndex,minDist**2\n",
    "        #print centroids\n",
    "        for cent in range(k):#recalculate centroids\n",
    "            ptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]#get all the point in this cluster\n",
    "            centroids[cent,:] = mean(ptsInClust, axis=0) #assign centroid to mean \n",
    "    return centroids, clusterAssment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
