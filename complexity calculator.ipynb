{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this part, we try to calculate the difficulty of a text, according to following formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 含雾指数 = (每句平均字数 + 难字数 )  *0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import codecs\n",
    "import jieba.posseg as pseg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/89/qp8kb90s49z7xcdskq2c0qlm0000gn/T/jieba.cache\n",
      "Loading model cost 0.714 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "# 创建停用词list\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "# 对句子进行分词\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stopwords = stopwordslist('stoped.txt')  # 这里加载停用词的路径\n",
    "    outstr = ''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分句子\n",
    "# 版本为python3，如果为python2需要在字符串前面加上u\n",
    "import re\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('([，！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)    \n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './dataset/数据/一千零一夜.txt'\n",
    "text = open(file_path, 'r',encoding=\"utf-8\")\n",
    "corpus = text.readlines()[1::2]\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for c in corpus: \n",
    "    #print(type(c))\n",
    "    #file_name='过年'\n",
    "    #output_name =  file_name+' 句子长度.xlsx'\n",
    "    #file_path = './dataset/raw_texts/'+file_name+'.txt'\n",
    "    #text = open(file_path, 'r',encoding=\"utf-8\") #加载要处理的文件的路径\n",
    "    df =pd.read_excel('./dataset/难字词库.xlsx') \n",
    "    diff_words = df['词语']\n",
    "    result_words = []\n",
    "    sent_arr = []\n",
    "    words_arr = []\n",
    "    sents = cut_sent(c)\n",
    "    #print(sents)\n",
    "    #print(block)  \n",
    "    for sent in sents:\n",
    "        words=list(seg_sentence(sent).split(' '))\n",
    "        #print(words)\n",
    "        sent_arr.append(len(sent))\n",
    "        for w in words:\n",
    "            #print(w)\n",
    "            words_arr.append(w)\n",
    "            if w in list(diff_words):\n",
    "                #print(w)     \n",
    "                if w not in result_words:\n",
    "                    #print(w)\n",
    "                    result_words.append(w)\n",
    "\n",
    "    sents_mean = np.mean(sent_arr) #平均每句字数\n",
    "    #print(sents_mean)\n",
    "    diff_word_count=len(result_words) #文本难词数\n",
    "    text_words_count=len(words_arr) #文本总词数\n",
    "    diff_ratio = diff_word_count/text_words_count #文本难字率\n",
    "    #print(diff_ratio)\n",
    "    arr.append((sents_mean + diff_ratio))\n",
    "    text_diff = (sents_mean + diff_ratio)*0.7  #含雾指数\n",
    "    #print(':'+str(text_diff))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n"
     ]
    }
   ],
   "source": [
    "print(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.258668346104054, 14.393126808270411, 16.20527972955058, 17.812223988694576, 13.216470588235293, 15.409027777777778, 13.937342767295599, 12.901008303677342, 12.601189351414439, 12.339203612479475, 16.163923440968297, 12.942268627610302, 11.543070728572577, 12.285415705637469, 13.793120393120393, 11.007236572825164, 13.63465981553703, 14.47623954599761, 12.00503379119236, 13.72484578824957, 14.361802206904711, 13.49278473728476, 12.113451230938798, 12.918711102467215, 14.53250996596101, 12.266090617037776, 15.110102403950904, 12.347065598157302, 14.35918600849951, 13.401923018408642, 13.02803738317757, 14.481760435571688, 14.567038539553753, 12.668961108439413, 15.928759148369926, 13.415584415584416, 11.962749169435217, 14.84828188701223, 14.677043620629165, 12.456920482743893, 14.830310122179485, 14.776530612244898, 12.817437533227007, 15.25188705619234, 15.741592312971859, 16.404871794871795, 22.679585798816568, 58.0, 15.60210897396252, 12.711130338252579, 13.13740477464394, 12.163852209406363, 14.378970852824757, 14.428620322482507, 14.969883908584517, 13.86334428102242, 12.947295458142111, 14.203883693388327, 9.298076923076923, 59.0, 12.968535419382878, 9.577827499483302, 8.914250604093935, 14.33915321438686, 9.300747329672037, 10.686916361786844, 9.413373983508347, 8.743024580875845, 13.336329632145413, 9.785627654772199, 14.06876602669472, 9.745162361480466, 12.29390874660838, 12.295039875481828, 12.74353098936216, 10.742273068808208, 15.590277777777779, 16.532822640706456, 9.022595783496142, 9.32314821090009, 13.377194889259231, 10.285671168032838, 13.193464994066792, 9.2695851720457, 13.72106587010654, 12.879556006691855, 12.830256682637067, 14.000994642392406, 13.197612586792317, 13.329170943361015, 14.352936874957953, 13.185617685954538, 13.686237713947799, 12.203988536349607, 13.227086610373217, 16.85452869074737, 14.987605078057985, 16.862946089597568, 15.137709137709138, 15.446498898604899, 9.401696357371131]\n"
     ]
    }
   ],
   "source": [
    "print(arr[0:101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.611842105263158"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_mean = np.mean(sent_arr) #平均每句字数\n",
    "sents_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_word_count=len(result_words) #文本难字数\n",
    "diff_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.528289473684207"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_diff = (sents_mean + diff_word_count)*0.7  #含雾指数\n",
    "text_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":74.67864638803137\n"
     ]
    }
   ],
   "source": [
    "# test code \n",
    "#file_name='班主任'\n",
    "#file_path = './dataset/raw_texts/'+file_name+'.txt'\n",
    "#text = open(file_path, 'r',encoding=\"utf-8\") #加载要处理的文件的路径\n",
    "#df =pd.read_excel('./dataset/难字词库.xlsx') \n",
    "diff_words = df['词语']\n",
    "result_words = []\n",
    "sent_arr = []\n",
    "words_arr = []\n",
    "for c in corpus:\n",
    "    #print(type(block))\n",
    "    sents = cut_sent(c)\n",
    "    #print(block)  \n",
    "    for sent in sents:\n",
    "        words=list(seg_sentence(sent).split(' '))\n",
    "        #print(words)\n",
    "        sent_arr.append(len(sent))\n",
    "        for w in words:\n",
    "            #print(w)\n",
    "            words_arr.append(w)\n",
    "            if w in list(diff_words):\n",
    "                #print(w)     \n",
    "                if w not in result_words:\n",
    "                    #print(w)\n",
    "                    result_words.append(w)\n",
    "\n",
    "sents_mean = np.mean(sent_arr) #平均每句字数\n",
    "#print(sents_mean)\n",
    "diff_word_count=len(result_words) #文本难词数\n",
    "text_words_count=len(words_arr) #文本总词数\n",
    "diff_ratio = diff_word_count/text_words_count #文本难字率\n",
    "#print(diff_ratio)\n",
    "text_diff = (sents_mean + diff_ratio)*0.7  #含雾指数\n",
    "print(':'+str(text_diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 笔画数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stroke(c):\n",
    "    # 如果返回 0, 则也是在unicode中不存在kTotalStrokes字段\n",
    "    strokes = []\n",
    "    with open(strokes_path, 'r') as fr:\n",
    "        for line in fr:\n",
    "            strokes.append(int(line.strip()))\n",
    "\n",
    "    unicode_ = ord(c)\n",
    "\n",
    "    if 13312 <= unicode_ <= 64045:\n",
    "        return strokes[unicode_-13312]\n",
    "    elif 131072 <= unicode_ <= 194998:\n",
    "        return strokes[unicode_-80338]\n",
    "    else:\n",
    "        print(\"c should be a CJK char, or not have stroke in unihan data.\")\n",
    "        # can also return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
