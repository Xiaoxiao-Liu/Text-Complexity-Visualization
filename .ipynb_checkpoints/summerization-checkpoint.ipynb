{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import codecs\n",
    "import jieba.posseg as pseg \n",
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyltp import Segmentor\n",
    "from pyltp import SentenceSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "_segmentor = None\n",
    "_sent_splitter = None\n",
    "\n",
    "def split(content):\n",
    "    '''分句和分词'''\n",
    "    global _segmentor, _sent_splitter\n",
    "    if _segmentor is None:\n",
    "        model_path = r'D:\\mysites\\text-characters\\tcharacters\\ltp\\ltp_data\\cws.model'\n",
    "        segmentor = Segmentor()  # 初始化实例\n",
    "        segmentor.load(model_path) # 加载分词模型\n",
    "        _segmentor = segmentor  # 设置全局变量, 避免每次都重新加载模型, 耗时\n",
    "        _sent_splitter = SentenceSplitter() # 句子分割模型\n",
    "    sents = _sent_splitter.split(content)  # 先进行分句\n",
    "    _sents = []\n",
    "    for sent in sents:\n",
    "        words = _segmentor.segment(sent) # 分词\n",
    "        sent = ' '.join(words) # 用空格把词隔开\n",
    "        _sents.append(sent)\n",
    "    content = '. '.join(_sents)  # 用.把句子隔开\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = '本模块提供文本摘要的功能。基于TextRank算法来计算文本句子的等级。'\n",
    "result = split(content)\n",
    "print(f'转换前: {content}')\n",
    "print(f'转换后: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "# 创建停用词list\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "# 对句子进行分词\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stopwords = stopwordslist('stoped.txt')  # 这里加载停用词的路径\n",
    "    outstr = ''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分句子\n",
    "# 版本为python3，如果为python2需要在字符串前面加上u\n",
    "import re\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('([，！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)    \n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换前: 本模块提供文本摘要的功能。基于TextRank算法来计算文本句子的等级。\n",
      "转换后: . \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = '''\n",
    "使用51篇文章的Opinion数据集进行比较。\n",
    "每篇文章都是关于产品的功能，例如iPod的电池寿命等，并且是购买该产品的客户的评论集合。\n",
    "数据集中的每篇文章都有5个手动编写的“黄金”摘要。\n",
    "通常5金总结是不同的，但它们也可以是相同的文本重复5次。\n",
    "LexRank是这里的赢家，因为它产生了更好的ROUGE和BLEU分数。\n",
    "不幸的是，我们发现由Gensim的TextRank和Luhn模型产生的摘要信息比摘要要少。\n",
    "此外，LexRank并不总是在ROUGE评分中击败TextRank  - 例如，TextRank在DUC 2002数据集上的表现稍好于LexRank。\n",
    "因此，LexRank和TextRank之间的选择取决于您的数据集，值得一试。\n",
    "数据的另一个结论是Gensim的Textrank优于普通的PyTextRank。\n",
    "因为它在明文TextRank中使用BM25函数而不是余弦IDF。\n",
    "表中的另一点是Luhn的算法具有较低的BLEU分数。\n",
    "这是因为它提取了更长的摘要，因此涵盖了更多的产品评论。\n",
    "不幸的是，我们不能缩短它，因为Sumy中Luhn算法的封装不提供参数来改变字数限制。\n",
    "'''\n",
    "#tokens = split(content)\n",
    "#summarize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = cut_sent(content)\n",
    "new_sents = []\n",
    "results = \"\"\n",
    "for s in sentences:\n",
    "    #print(s)\n",
    "    \n",
    "    #result = seg_sentence(s)+s[-1]\n",
    "    #results+=result\n",
    "    if s =='':\n",
    "        result = s\n",
    "    else:\n",
    "        result = seg_sentence(s)[:-1]+s[-1]+'\\n'\n",
    "    results+=result\n",
    "        \n",
    "        #seg_sentence(stri)\n",
    "        #new_sents.append(s)\n",
    "        \n",
    "#new_sents[0][-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'使用 51 篇文章 Opinion 数据 集 进行 比较。\\n每篇 文章 都 产品 功能，\\niPod 电池 寿命，\\n购买 产品 客户 评论 集合。\\n数据 集中 每篇 文章 都 手动 编写 黄金 摘要。\\n通常 金 总结 不同，\\n相同 文本 重复 次。\\nLexRank 赢家，\\n产生 更好 ROUGE BLEU 分数。\\n不幸，\\n发现 Gensim TextRank Luhn 模型 产生 摘要 信息 摘要 要少。\\n，\\nLexRank 不 总是 ROUGE 评分 中 击败 TextRank      ，\\nTextRank DUC   2002 数据 集上 表现 稍 好 LexRank。\\n，\\nLexRank TextRank 之间 选择 取决于 数据 集，\\n值得 一试。\\n数据 结论 Gensim Textrank 优于 普通 PyTextRank。\\n明文 TextRank 中 使用 BM25 函数 不是 余弦 IDF。\\n表中 一点 Luhn 算法 具有 低 BLEU 分数。\\n是因为 提取 更长 摘要，\\n涵盖 更 产品 评论。\\n不幸，\\n不能 缩短，\\nSumy 中 Luhn 算法 封装 不 提供 参数 改变 字数 限制。\\n'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = split(content)\n",
    "#tokens = seg_sentence(content)\n",
    "tokens = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'使用 51 篇文章 Opinion 数据 集 进行 比较。\\n每篇 文章 都 产品 功能，\\niPod 电池 寿命，\\n购买 产品 客户 评论 集合。\\n数据 集中 每篇 文章 都 手动 编写 黄金 摘要。\\n通常 金 总结 不同，\\n相同 文本 重复 次。\\nLexRank 赢家，\\n产生 更好 ROUGE BLEU 分数。\\n不幸，\\n发现 Gensim TextRank Luhn 模型 产生 摘要 信息 摘要 要少。\\n，\\nLexRank 不 总是 ROUGE 评分 中 击败 TextRank      ，\\nTextRank DUC   2002 数据 集上 表现 稍 好 LexRank。\\n，\\nLexRank TextRank 之间 选择 取决于 数据 集，\\n值得 一试。\\n数据 结论 Gensim Textrank 优于 普通 PyTextRank。\\n明文 TextRank 中 使用 BM25 函数 不是 余弦 IDF。\\n表中 一点 Luhn 算法 具有 低 BLEU 分数。\\n是因为 提取 更长 摘要，\\n涵盖 更 产品 评论。\\n不幸，\\n不能 缩短，\\nSumy 中 Luhn 算法 封装 不 提供 参数 改变 字数 限制。\\n'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'发现 Gensim TextRank Luhn 模型 产生 摘要 信息 摘要 要少。\\nLexRank 不 总是 ROUGE 评分 中 击败 TextRank      ，\\nLexRank TextRank 之间 选择 取决于 数据 集，\\n表中 一点 Luhn 算法 具有 低 BLEU 分数。'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
