{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this part, we try to calculate the difficulty of a text, according to following formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 含雾指数 = (每句平均字数 + 难字数 )  *0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import codecs\n",
    "import jieba.posseg as pseg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/89/qp8kb90s49z7xcdskq2c0qlm0000gn/T/jieba.cache\n",
      "Loading model cost 0.722 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "# 创建停用词list\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "# 对句子进行分词\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stopwords = stopwordslist('stoped.txt')  # 这里加载停用词的路径\n",
    "    outstr = ''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分句子\n",
    "# 版本为python3，如果为python2需要在字符串前面加上u\n",
    "import re\n",
    "def cut_sent(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('([，！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)    \n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_arr = ['小美人鱼','远方飞来了一只红蜻蜓','金童求雨记','过年','产业基础高级化 发展迈向高质量','冯骥才-这个时代文化的使命首先是抢救','开市客火爆背后，美国零售业经历过的惨烈搏杀','恋爱的犀牛剧本','班主任','温柔的夜-三毛']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小美人鱼:7.233524089819053\n",
      "远方飞来了一只红蜻蜓:6.02\n",
      "金童求雨记:5.549124409245377\n",
      "过年:8.148741000908645\n",
      "产业基础高级化 发展迈向高质量:9.682413785133136\n",
      "冯骥才-这个时代文化的使命首先是抢救:8.226086983102242\n",
      "开市客火爆背后，美国零售业经历过的惨烈搏杀:10.827965367965367\n",
      "恋爱的犀牛剧本:6.060491222586862\n",
      "班主任:8.669599245398459\n",
      "温柔的夜-三毛:4.787560420698086\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_arr:   \n",
    "    #file_name='过年'\n",
    "    #output_name =  file_name+' 句子长度.xlsx'\n",
    "    file_path = './dataset/课文/'+file_name+'.txt'\n",
    "    text = open(file_path, 'r',encoding=\"utf-8\") #加载要处理的文件的路径\n",
    "    df =pd.read_excel('./dataset/难字词库.xlsx') \n",
    "    diff_words = df['词语']\n",
    "    result_words = []\n",
    "    sent_arr = []\n",
    "    words_arr = []\n",
    "    for block in text:\n",
    "        #print(type(block))\n",
    "        sents = cut_sent(block)\n",
    "        #print(block)  \n",
    "        for sent in sents:\n",
    "            words=list(seg_sentence(sent).split(' '))\n",
    "            #print(words)\n",
    "            sent_arr.append(len(sent))\n",
    "            for w in words:\n",
    "                #print(w)\n",
    "                words_arr.append(w)\n",
    "                if w in list(diff_words):\n",
    "                    #print(w)     \n",
    "                    if w not in result_words:\n",
    "                        #print(w)\n",
    "                        result_words.append(w)\n",
    "\n",
    "    sents_mean = np.mean(sent_arr) #平均每句字数\n",
    "    #print(sents_mean)\n",
    "    diff_word_count=len(result_words) #文本难词数\n",
    "    text_words_count=len(words_arr) #文本总词数\n",
    "    diff_ratio = diff_word_count/text_words_count #文本难字率\n",
    "    #print(diff_ratio)\n",
    "    text_diff = (sents_mean + diff_ratio)*0.7  #含雾指数\n",
    "    print(file_name+':'+str(text_diff))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.611842105263158"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_mean = np.mean(sent_arr) #平均每句字数\n",
    "sents_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_word_count=len(result_words) #文本难字数\n",
    "diff_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.528289473684207"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_diff = (sents_mean + diff_word_count)*0.7  #含雾指数\n",
    "text_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.35064935064935\n",
      "0.034492428491306786\n",
      "班主任:8.669599245398459\n"
     ]
    }
   ],
   "source": [
    "# test code \n",
    "file_name='班主任'\n",
    "file_path = './dataset/课文/'+file_name+'.txt'\n",
    "text = open(file_path, 'r',encoding=\"utf-8\") #加载要处理的文件的路径\n",
    "df =pd.read_excel('./dataset/难字词库.xlsx') \n",
    "diff_words = df['词语']\n",
    "result_words = []\n",
    "sent_arr = []\n",
    "words_arr = []\n",
    "for block in text:\n",
    "    #print(type(block))\n",
    "    sents = cut_sent(block)\n",
    "    #print(block)  \n",
    "    for sent in sents:\n",
    "        words=list(seg_sentence(sent).split(' '))\n",
    "        #print(words)\n",
    "        sent_arr.append(len(sent))\n",
    "        for w in words:\n",
    "            #print(w)\n",
    "            words_arr.append(w)\n",
    "            if w in list(diff_words):\n",
    "                #print(w)     \n",
    "                if w not in result_words:\n",
    "                    #print(w)\n",
    "                    result_words.append(w)\n",
    "\n",
    "sents_mean = np.mean(sent_arr) #平均每句字数\n",
    "print(sents_mean)\n",
    "diff_word_count=len(result_words) #文本难词数\n",
    "text_words_count=len(words_arr) #文本总词数\n",
    "diff_ratio = diff_word_count/text_words_count #文本难字率\n",
    "print(diff_ratio)\n",
    "text_diff = (sents_mean + diff_ratio)*0.7  #含雾指数\n",
    "print(file_name+':'+str(text_diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 笔画数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stroke(c):\n",
    "    # 如果返回 0, 则也是在unicode中不存在kTotalStrokes字段\n",
    "    strokes = []\n",
    "    with open(strokes_path, 'r') as fr:\n",
    "        for line in fr:\n",
    "            strokes.append(int(line.strip()))\n",
    "\n",
    "    unicode_ = ord(c)\n",
    "\n",
    "    if 13312 <= unicode_ <= 64045:\n",
    "        return strokes[unicode_-13312]\n",
    "    elif 131072 <= unicode_ <= 194998:\n",
    "        return strokes[unicode_-80338]\n",
    "    else:\n",
    "        print(\"c should be a CJK char, or not have stroke in unihan data.\")\n",
    "        # can also return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
